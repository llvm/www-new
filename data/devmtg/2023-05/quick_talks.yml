quick_talks:
  - title: "Iterative Compilation - Give the compiler a second chance"
    speaker: "Ziv Ben Zion, Mobileye"
    video_url: "https://youtu.be/XMuS2PGyWms"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May10/06-Zion-Iterative%20Compilation%20-%20EuroLLVM.pdf"
    description: |
      Compiler heuristics play a crucial role in improving the performance of generated code.
      Revisiting some of the decisions taken by the compiler is possible using different
      compilation flags, and can sometimes overcome wrong compiler decisions. This talk
      introduces a different approach, where the compiler itself triggers new compiler runs with
      different heuristics. I will briefly outline how we implemented this new approach in our
      LLVM-based compiler.

  - title: "Another level of indirection - Compiler Fallback of load/store into gather/scatter enhance compiler robustness by overcoming analysis and hardware limitations"
    speaker: "Omer Aviram, Mobileye"
    video_url: "https://youtu.be/85VX3CQTFt8"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May11/06%20-Aviram-PDF%20EuroLLVM%20presentation%20-%20Fallback.pdf"
    description: |
      I'll introduce a recently developed LLVM-IR utility that can improve compilers robustness,
      by converting ("fallback") memory accesses (load/store) with a constant stride, into
      indirect accesses (scatter/gather); Discuss interesting cost decisions raised by such
      transformations; as well as the technical challenges faced in transforming load/store
      instructions with a single pointer into gather/scatter instructions with a vector of
      pointers.

  - title: "Switch per function in LLVM"
    speaker: "Tomer Nissim Schneider, CEVA"
    video_url: "https://youtu.be/-cPmbtw9dlQ"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May11/05-Schneider-SwitchPerFunction_FINAL.pdf"
    description: |
      At CEVA, we have found that more optimizations and compiler hints are extremely essential
      for optimizing code of our customers.​ We added support for clang switches as function
      attributes.

  - title: "Tensor Evolution - An ML Graph Optimization Technique"
    speaker: "Javed Absar, Qualcomm & Muthu Baskaran, Qualcomm"
    video_url: "https://youtu.be/T1yPnrVgISo"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May10/03-Absar-Tensor_Evolution_Final_Slides.pdf"
    description: |
      We present 'Tensor Evolution (TEV)', a new analysis for tensors such as those found in
      loops of ML Graphs. It is an extension of the well-known Scalar Evolution (SCEV) for
      tensors and tensor expressions. In an ML Graph, tensors can be added, multiplied, sliced,
      reshaped, and concatenated. We describe how each of these tensor-ops could be handled to
      generate TEV-expressions and rewrite rules. TEV is an analysis that enables optimizations
      such as loop-invariant code motion.

  - title: "ML-on-CPU: should vectorization happen in the LLVM backend or higher up the stack? "
    speaker: "Elen Kalda, Arm"
    video_url: "https://youtu.be/hyi1rtSHI6g"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May10/02-Kalda-euroLLVM_ML_on_CPU.pdf"
    description: |
      This talk is about how TVM, one of the most mature machine learning compilation stacks in
      ML space, interacts with LLVM. TVM is a domain specific compiler that consumes a machine
      learning model expressed in high level ML framework like TensorFlow or PyTorch and
      compiles it for a chosen target, such as Arm(R) architecture. For CPU targets, it does
      this by using LLVM as a backend, directly translating TVM's IR into LLVM IR. 
      In TVM, just like in other Machine Learning stacks using LLVM as a backend for CPU
      code generation, one needs to make a decision about where optimizations like vectorization
      should happen: in the LLVM backend, or in the ML stack higher up. This is further
      complicated by the emergence of scalable vectors, like the Scalable Vector Extension
      (SVE). While generating code for fixed length vectors can mostly be left to LLVM, there is
      a case to be made for the presence of variable length vectors in TVM stack, to be able to
      more effectively use the capabilities of SVE. In this talk, we're going to present our
      experiences and insights on the trade-offs targeting SVE in the TVM+LLVM stack.

  - title: "CORE-V LLVM: Adding eight vendor extensions to standard RISC-V LLVM"
    speaker: "Charlie Keaney & Chunyu Liao, Embecosm"
    video_url: "https://youtu.be/Sjg6xW6bQuM"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May11/04-Charlie-Keaney-Presentation.pdf"
    description: |
      CORE-V is a family of open source commercially rust RISC-V designs from the Open Hardware
      Group, with a set of 8 custom instruction set extensions. This talk will look at the
      practical challenges we have encountered in supporting vendor specific extensions to
      RISC-V in LLVM. This has been a collaborative project across several organizations on two
      continents, and with an additional objective of training a new generation of LLVM
      developers in China and Europe.

  - title: "Advanced Bug Reports: Choose Your Own Adventure"
    speaker: "Arseniy Zaostrovnykh, SonarSource SA"
    video_url: "https://youtu.be/XAXFimqCCwI"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May11/03%20-arseniy-zaostrovnykh-path-sensitive-bug-reports-choose-your-own-adventure-slides.pdf"
    description: |
      Finding actual bugs using the Clang Static Analyzer (CSA) is only half of the story.
      Getting bugs fixed also requires convincing developers that those bugs are real.
      Traditional bug reports, however, are typically either too short and miss important
      details, or too long such that they overwhelm developers with information. This talk
      presents a novel approach to make CSA-bug-reports interactive to confront developers with
      exactly the amount of information they need to understand and confirm a bug.

  - title: "Multiple-Entry, Multiple-Exit MLIR Regions"
    speaker: "Jeff Niu, Modular"
    video_url: "https://youtu.be/TM-97iiozDY"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May10/01%20-Multiple-Exit%20MLIR%20Blocks-EuroLLVM%202023.pdf"
    description: |
      MLIR regions provide a natural representation of structured control-flow found in many
      applications, with implicit SSA value captures and automatic memory scopes, but they have
      been limited to single-entry, single-exit regions. In this talk, we present a new MLIR
      region-based control-flow representation for single-entry, multiple-exit regions and how
      this provides a faithful IR model of control-flow in source languages. We also integrated
      LLVM coroutine intrinsics in our compiler, and we will discuss how they interact with our
      control-flow representation and how the latter enables trivial implementations of
      coroutine frame optimizations.

  - title: "Target-Independent Integer Arithmetic"
    speaker: "Jeff Niu, Modular"
    video_url: "https://youtu.be/PBA_iHGc6Zg"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May11/01%20-Target-Independent%20Integer%20Arithmetic.pdf"
    description: |
      How can you fold address arithmetic without knowing the maximum machine integer size? How
      about C integer types with variable widths? What about range analysis? Folding target-
      independent IR is important in producing target-agnostic serialization, where introducing
      a target can cause invalid arithmetic semantics. This talk will present a formulation for
      target-independent integer arithmetic, its limitations, and how it was implemented in
      MLIR.

  - title: "Improving Vectorization for Loops with Control Flow"
    speaker: "Ashutosh Nema, AMD"
    video_url: "https://youtu.be/mKG0NmGtpbE"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May10/05-EuroLLVM23%20-%20Improving%20Vectorization%20for%20Loops%20with%20Control%20Flow.pdf"
    description: |
      Auto-vectorization is an essential compiler optimization. In the presence of control flow,
      it gets challenging. We introduce the implementation of Branch-On-Super-Word-Conditional-
      Codes (BOSCC) way of vectorization in the presence of conditional statements. BOSCC
      introduces a branch instruction that can be conditionally taken based on the comparison
      result of two vector variables. BOSCC encloses the vector instructions guarded by vector
      predicate inside an if-statement.

  - title: "How to run the LLVM-Test Suite on GPUs and what you’ll find"
    speaker: "Johannes Doerfert, LLNL"
    video_url: "https://youtu.be/_IYG_aMjsfs"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May11/02%20-EuroLLVM%2023%20-%20LLVM%20TS%20on%20GPU.pdf"
    description: |
      Running codes on GPUs is nowadays pretty easy. However, testing GPU compilation on a
      large, well-understood selection of codes is not. We present an automated approach that
      allows running (most) existing codes on the GPU in order to test the optimizations and
      backends. We present our findings from running (most of) the LLVM Test Suite on modern
      GPUs and show how we combine existing functionality to create concise GPU reducers for
      bugs.

  - title: "OpenMP as GPU Kernel Language"
    speaker: "Johannes Doerfert, LLNL"
    video_url: "https://youtu.be/16cedvl2Ly4"
    slides_url: "https://llvm.org/devmtg/2023-05/slides/QuickTalks-May10/04%20-%20EuroLLVM%2023%20-%20OpenMP%20Kernel%20Language.pdf"
    description: |
      In this talk, we discuss the use of OpenMP as a kernel language (think CUDA or HIP). While
      OpenMP comes with offloading capabilities, the execution model was different and generally
      associated with overheads. Further, the user did not have the same level of control, at
      least not without target-specific builtins. With our new OpenMP extensions, we can match
      native CUDA (and HIP) codes while retaining the portability of OpenMP as well as
      interoperability with the existing capabilities.