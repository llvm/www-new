posters:
- title: "Improving Machine Code Generation Quality by Interfacing VPO with LLVM"
  speaker: "Ryan Baird"
  slides_url: "https://llvm.org/devmtg/2013-04/baird-poster.pdf"
  video_url: ""
  description: "Very Portable Optimizer (VPO) is a research compiler backend that performs optimizations on a single intermediate representation called Register Transfer Lists (RTLs). RTLs are at the level of machine instructions, and therefore most of VPO's code improving optimizations can be performed in a machine independent way and optimization phases can be repeated in an arbitrary order. For these merits, VPO is widely used to optimize machine code that exploits various architecture features on low-power embedded processors. However, VPO uses LCC as a front-end, which does not support current language standards (C99,C++98) and has no mid-level code improving transformations. The contributions of this poster are two-fold. First, it describes our approach to extend both the code coverage and the quality of VPO machine code generation by streamlining the compiler with LLVM instead of LCC. Second, it provides some insight to the LLVM community into alternative machine code generation, which can be effectively achieved by interfacing with an existing optimizing compiler instead of creating a different machine port within LLVM."

- title: "Using the LLVM Interpreter to Quantify Applications Inherent Properties"
  speaker: "Victoria Caparros"
  slides_url: "https://llvm.org/devmtg/2013-04/caparros-poster.pdf"
  video_url: ""
  description: "This poster presents a tool based on the LLVM interpreter for quantifying application's inherent properties. Our approach to quantifying applications properties is based on previous studies that use a microarchitectural simulator to emulate a machine with unlimited hardware resources, and quantify application behavior from the analysis of the data dependences and data movement properties of the dynamic instruction trace during execution on the simulator [2]. This approach has several advantages. First, application properties are measured for the particular input considered, as opposed to the theoretical analysis of the algorithm, which does not consider input size. Second, it provides a better insight into application behavior, since it exposes a broader range of application properties, not only those that are exploitable with existing microarchitectural features (what can be measured with hardware performance counters on a target platform), but also properties that may require new hardware features in order to be exploited. Finally, this approach enables us to reason about application's performance across different platforms with just a single pass of the analysis, not requiring to repeat the analysis for every hardware configuration of interest."

- title: "MCLinker: Design and Implementation of a Fragments-based Target-independent Linker"
  speaker: "Diana Chen"
  slides_url: "https://llvm.org/devmtg/2013-04/chen-poster.pdf"
  video_url: ""
  description: "MCLinker is a system linker that uses fragments as its internal intermediate representations (IRs) to process inputs, such as a .o file, and generate desired output file. Valid inputs for now are: .o, .a, .so, and a piece of in-memory of code; and valid outputs are: .o, .so, and executable binary. Fragments are good IRs for a linker in a way that these fragments can easily generate data structures to be used by linkers. Fragments can be either a function, a block of code, or a defined symbol with a memory region. For instance, the global offset table in a typical .so and the frame description table in DWARF format of MCLinker are made directly from fragments. Another reason is linking finer-grained fragments could lead to a more optimized result as opposed to coarse-grained sections, because finer-grained fragments would facilitate better data-stripping and reordering. However, LLVM MC fragments are originally designed for assembler so they could not fulfill some requirements from linkers. Thus, MCLinker needs to define some additional fragment types. For example, MCLinker defines general relocation fragments and stub fragments to represent stubs of the branch islands. MCLinker also defines “region fragment” to hold arbitrary blocks of code or data. Furthermore, MCLinker introduces a general reference linkage between two fragments to represent their relocation relationship. MCLinker is a full-fledged system linker. It is capable of linking ELF object files on various platforms, such as ARM, x86, and MIPS. Some additional targets, such as x86-64 and x32, are still in development and will be available soon. MCLinker fully supports complex ELF features, such as DWARF debugging, Itanium exception handling, COMDAT sections, instruction relaxation, and GNU archive format."

- title: "Code Editing in Local Style"
  speaker: "Peter Conn"
  slides_url: "https://llvm.org/devmtg/2013-04/conn-poster.pdf"
  video_url: ""
  description: "Coding styles contain a variety of elements, from indenting rules to variable and function names. They also place different constraints on variable declarations, for example requiring them at the start of the function or in the smallest possible scope. Some of these can be automatically checked with purely syntactic checkers, such as typical implementations of the UNIX indent tool. Others require semantic knowledge. For example, moving a variable declaration requires knowing where all of its uses are. The goal of the CELS (Code Editing in Local Style) tool is to allow each developer to edit code in their preferred style, while preserving a uniform style in the repository. It includes the ability to specify complex styles, to infer styles from an existing corpus of source code, and to perform automatic formatting. CELS is written as a library on top of libclang, allowing it to be embedded in code editors and IDEs without relying on unstable binary interfaces. It traverses the AST exposed by libclang and builds a scope tree, renames symbols, moves declarations, and wraps lines. The line-wrapping algorithm used is based on the TEX line breaking algorithm. The user may specify different penalties for different line breaking locations, for example encoding rules such as `prefer to break after an operator, try to avoid breaking before a comma, breaking after a semicolon is best' and have the lines wrapped accordingly. Additionally, the typesetter is aware of the distinction between whitespace used for indentation and whitespace used for alignment and so can use different characters for either, for example using tabs for indentation and spaces for alignment, allowing the resulting code to be viewed with any tab size without losing alignment."

- title: "ENHANCE - Enabling heterogeneous hardware acceleration using novel programming and scheduling models"
  speaker: "Dustin Feld"
  slides_url: "https://llvm.org/devmtg/2013-04/feld-poster.pdf"
  video_url: ""
  description: "Developers faced with the task of creating parallel applications on heterogeneous computing architectures often fail to reach acceptable performance and speed-up values due to sub-optimal communication patterns in their application. At the same time, the operating system often is not able to reach optimal resource utilization, due to missing possibilities for relocating user threads and user processes across hardware boundaries. Some of these issues can already be tackled at compile time if the compiler is able to understand coding and design patterns and acts accordingly. In this presentation we introduce an automatic framework for parallelization, check-pointing, and task scheduling based on the LLVM compiler framework. Our work includes techniques which facilitate an efficient usage of heterogeneous resources with a dynamic and automated approach. Furthermore, a task scheduling framework on a single node basis takes care of the fair use scheduling of available hardware resource in a multi-user environment."

- title: "Sambamba: A Runtime System for Online Adaptive Parallelization"
  speaker: "Clemens Hammacher"
  slides_url: "https://llvm.org/devmtg/2013-04/hammacher-poster.pdf"
  video_url: ""
  description: "Automatic parallelization is a classical compiler problem: Using static analyses, the compiler tries to prove computations independent from each other, and estimates the benefit that would be gained by executing these codes in parallel. Both of these tasks represent huge challenges, as neither the input data nor characteristics of the execution platform are typically known at compile time. However, those factors mainly determine where parallelization is applicable and beneficial. Because of this discrepancy, the effectiveness of parallelizing compilers is very limited. On the other hand, manual parallelization has also proven to be a serious hurdle for the majority of developers. A lot of new languages and programming libraries have been built to support programmers in that task, but all of them still require expertise in order to build efficient applications. Sometimes runtime support is installed, such as software transactional memory, to enable speculative parallelization where the independence of computations can not be proven. This makes it even harder to estimate whether the overhead will pay off at runtime, in the sense of an overall performance improvement. Therefore, we propose an automatic runtime-adaptive system. It executes the target application in a lightweight virtual machine, and constantly monitors its runtime behaviour. This information is then used to decide where and how to parallelize. This way, alternative variants of individual functions are provided, which have been optimized for the observed input. Those variants do not immediately replace previous code by installing them into the running application, but it is the runtime system's responsibility to identify the best performing variant for the situation at hand."

- title: "OJIT: A novel secure remote execution technology by obfuscated Just-In-Time compilation	"
  speaker: "Muhammad Hataba"
  slides_url: "https://llvm.org/devmtg/2013-04/hataba-poster.pdf"
  video_url: ""
  description: "This poster presents the Obfuscating Just-In-Time compilation (OJIT) technique. OJIT is a novel security technique for a trustworthy and secured code execution on a remote premise such as the cloud-computing environment. We rely on the principles of obscurity for the sake of security, which is a concept widely popular in software protection. LLVM's just-in-time (JIT) compilation is used to dynamically obfuscate code, making the generated code unintelligible and hence difficult to reverse engineer. We obfuscate the code by an array of randomly yet dynamically changing techniques that are independent of the source language of the executed program yet neutral to the platform that we are executing on. We evaluated the technique by measuring a variety of obfuscation metrics running a set of benchmark programs."

- title: "LLVM backend for TILE64"
  speaker: "Dávid Juhász"
  slides_url: "https://llvm.org/devmtg/2013-04/juhasz-poster.pdf"
  video_url: ""
  description: "LLVM provides a platform-independent intermediate layer for developers of highlevel programming languages. Benefits of transforming high-level programs into LLVM IR are twofold: high-level to intermediate compilation does not have to deal with platform-specific details, still executables for many different architectures can be compiled using back-ends already implemented for the LLVM toolchain. Programs in platform-independent intermediate representation are to be analyzed and transformed in order to be optimized, e.g. run faster or consume less memory, and to generate executable code. The last major step of processing programs is code generation which can be done using LLVM back-ends. LLVM has a target-independent code generator, in which the most common features for turning a target-independent representation into a platform-specific assembly or binary are implemented. An LLVM back-end is mainly the parameterization of the target-independent code generator with platform-specific properties. In many cases, the default implementation of different parts of the code generation fits well to the target. However, overriding some features is necessary for architectures which are to some extents different from the mainstream ones. Tile64 was the first commercial product of Tilera Corporation back in 2007, that has been followed by other more sophisticated Tilera processors. Tile64 is an energy-efficient massively parallel processor architecture. It consists of 64 general purpose processor cores (tiles) connected by a mesh-network. Each core has 3 pipelines, two of them are for integer and logical operations and the third one is a load-store unit. The shortpipeline, in-order, three-issue (there are two-issue bundles as well) cores implement a VLIW instruction set supporting RISC instructions extended with various SIMD and DSP-related operations. The processor is very capable of communication both inside and outside the mesh-network. The speed of the interconnection between tiles is one hop per tick, and the edges of the mesh are connected to different I/O interfaces (four DDR2 controllers, two 10-gigabit Ethernet interfaces, two four-lane PCIe interfaces, and a software-configured flexible I/O interface). Tile64 is in many ways different from the mainstream processor architectures, thus implementing an LLVM back-end for Tile64 posed several questions. Some of those questions and tricky parts of implementation are to be revealed in the poster."

- title: "Noise: A Clang Extension for User-Defined Optimization Strategies"
  speaker: "Ralf Karrenberg"
  slides_url: "https://llvm.org/devmtg/2013-04/karrenberg-poster.pdf"
  video_url: ""
  description: "In this talk, we present Noise, a language extension to Clang that enables a programmer to control the optimization process on a code region. Noise is a language extension that allows a programmer to create custom optimization strategies and apply them to specific code segments. This enables fine-grained control over the optimizations applied by the compiler to conveniently tune code without actually rewriting it. With Noise, the programmer can easily choose optimizations and their order without interfering with the standard optimizations being applied to the remaining program. This is especially important for legacy code in the High-Performance Computing (HPC) environment, but is also relevant in other performance-sensitive fields such as computer graphics. We present our implementation for C/C++ within the Clang frontend using attributes. In addition to exposing LLVM's internal optimization phases, Noise also has special transformations built-in, for example data-parallel loop vectorization on the basis of Whole-Function Vectorization. We show first results demonstrating the effectiveness of Noise on HPC code in production."

- title: "LLVM IR editor plugin for Eclipse"
  speaker: "Ayal Zaks"
  slides_url: "https://llvm.org/devmtg/2013-04/zaks-poster.pdf"
  video_url: ""
  description: "LLVM IR SDK is an Eclipse plugin that adds an LLVM IR (.ll files) editor to Eclipse. Intended for LLVM developers, it is designed to provide nearly the same level of support for IR files that other programming languages enjoy under Eclipse, making it easier to write tests and analyze pass output. By incorporating a wide range of validation checks as you type - from simple syntax checks through full type validation and up to dominance analysis - the plugin enables a quick modify-and-run cycle for IR files, without having to run LLVM module validation in-between. In addition, the plugin exposes a variety of quick-fix options for common code actions, such as fixing broken number sequence for local names, inserting conversions between types, inserting function declarations inferred from a function call, and more."